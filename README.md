# ğŸ¯ You Just Want Attention

**Implementing Attention Is All You Need â€” From Scratch**
ğŸš€ German â†’ English Neural Machine Translation with PyTorch

---

## ğŸ“œ Overview

Welcome to **You Just Want Attention** â€” a minimalist yet powerful deep dive into implementing the core of modern NLP: **attention**.

In this project, we build a **Transformer** from scratch (yes, no cheating with Hugging Face's pretrained magic) to translate **German to English**, using the **WMT14 dataset**.

> â€œAttention is all you need.â€ â€” Vaswani et al., 2017
> â€œAlso, please tokenize your inputs.â€ â€” Your GPU

---

## ğŸ§  What You'll Learn

* How **scaled dot-product attention** works
* Why **multi-head attention** matters
* What are **positional encodings**, and why we need them
* How to build a Transformer encoder-decoder from scratch in PyTorch
* End-to-end training for German â†’ English translation
* How to evaluate your model with BLEU scores

---

## ğŸ“¦ Dataset

We use the **WMT14 English-German** translation dataset:

```python
from datasets import load_dataset

dataset = load_dataset("wmt14", "de-en", split="train")
```

Each example has:

```python
{
  "translation": {
    "de": "Das ist ein Beispiel.",
    "en": "This is an example."
  }
}
```

---

## ğŸ§± Architecture

Our implementation closely follows the original Transformer paper:

* `Input Embeddings` + `Positional Encoding`
* `Multi-Head Self-Attention`
* `Encoder-Decoder Attention`
* `Feed Forward Networks`
* `Layer Normalization + Residual Connections`

Everything's written from scratch using **PyTorch** â€” no hidden layers.

---

## ğŸš€ Training

```bash
python train.py
```

Training is done with:

* Adam Optimizer with Warmup LR Scheduler
* Label smoothing loss
* BLEU score monitoring

---

## ğŸ›  Requirements

* Python 3.8+
* PyTorch
* `datasets` from Hugging Face
* `tqdm`, `numpy`

Install dependencies:

```bash
pip install -r requirements.txt
```

---

## ğŸ“ˆ Results

Once trained, the model achieves decent translation quality, and BLEU improves over time as attention learns what to focus on â€” just like in life.

---

## ğŸ§ª Example Translation

```text
German:    Ich liebe maschinelles Lernen.
Predicted: I love machine learning.
```

---

## ğŸ¤– Inspiration

This project was inspired by:

* [Attention Is All You Need (Vaswani et al., 2017)](https://arxiv.org/abs/1706.03762)
* Curiosity to see *how attention really works* under the hood

---

## ğŸ’¬ Fun Fact

> â€œYou just want attentionâ€ â€” Charlie Puth
> He was clearly talking to his encoder.


---

## ğŸ“¬ Contact

Feel free to open issues or reach out if you want to collaborate!

---

## ğŸŒŸ Star This Repo

If you're learning NLP from scratch, this one's for you. Give it a â­ if it helped you understand attention better!
